"""Hydrocracker OFM Dashboard (Fuel Gas Soft Sensor + MSPC)

This dashboard supports two workflows:

1) Monitor: Explore an existing scores CSV (generated by `pipelines/score_ofm.py`).
2) Build/Score: Upload a dataset + model bundles and score *inside* the dashboard,
   then flip back to Monitor.

Key UI changes (requested):
- Remove the hard-coded "Latest run (missing) outputs/scores.csv" option. We now
  list only the scores CSVs that actually exist under /outputs.
- Advanced scoring controls only appear in the "Build / Score" tab.
- Clarify row counts: show both raw rows-in-window and plotted (downsampled) rows.
- Fix residual chart: plot standardized residual + EWMA around zero with limits.
- Replace the old OFM health index plot with a health+uncertainty plot derived from
  combined anomaly score.
- Add a dynamic "equipment failure trajectory" plot with symptom annotations.
- Fix MSPC chart by plotting T2 and SPE as ratio-to-limit.
- Control chart auto-zooms to the data range.
"""

from __future__ import annotations
import re
import io
import sys
from pathlib import Path
from typing import Optional

# -------------------- Make local package importable --------------------
# Streamlit runs with sys.path rooted at the script directory (dashboard/),
# so `src/` may not be discoverable unless the project is installed.
ROOT = Path(__file__).resolve().parents[1]
SRC_DIR = ROOT / "src"
if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

import joblib
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import streamlit as st

from ofm_fg_ofm.pipelines.score_ofm import score_dataframe


# -------------------- Paths --------------------
OUTPUTS_DIR = ROOT / "outputs"
OUTPUTS_DIR.mkdir(exist_ok=True)


# -------------------- Helpers --------------------
@st.cache_data(show_spinner=False)
def load_scores_csv(path: str | Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    return _ensure_scores_schema(df)


def load_scores_uploaded(file: st.runtime.uploaded_file_manager.UploadedFile) -> pd.DataFrame:
    df = pd.read_csv(file)
    return _ensure_scores_schema(df)


def _ensure_datetime(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
    elif "Timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["Timestamp"], format="mixed", dayfirst=True, errors="coerce")
    else:
        df["timestamp"] = pd.date_range("2020-01-01", periods=len(df), freq="1min")

    df = df.dropna(subset=["timestamp"]).sort_values("timestamp").reset_index(drop=True)
    return df


def _ensure_scores_schema(df: pd.DataFrame) -> pd.DataFrame:
    """Make uploaded/older score CSVs usable by this dashboard.

    This dashboard expects columns created by the updated `score_ofm` pipeline.
    If a CSV is missing some newer columns, we derive reasonable fallbacks.
    """
    df = _ensure_datetime(df)

    # Ensure actual/pred + residual
    if "y_actual" in df.columns and "y_pred" in df.columns:
        y = pd.to_numeric(df["y_actual"], errors="coerce")
        yhat = pd.to_numeric(df["y_pred"], errors="coerce")
    elif "y" in df.columns and "yhat" in df.columns:
        y = pd.to_numeric(df["y"], errors="coerce")
        yhat = pd.to_numeric(df["yhat"], errors="coerce")
        df["y_actual"] = y
        df["y_pred"] = yhat
    else:
        # Still allow MSPC-only CSVs
        y = pd.Series([np.nan] * len(df))
        yhat = pd.Series([np.nan] * len(df))
        if "y_actual" not in df.columns:
            df["y_actual"] = y
        if "y_pred" not in df.columns:
            df["y_pred"] = yhat

    if "residual" not in df.columns:
        df["residual"] = (y - yhat).to_numpy(dtype=float)

    # Basic EWMA (raw residual)
    if "residual_ewma" not in df.columns:
        df["residual_ewma"] = df["residual"].ewm(span=30, adjust=False).mean().to_numpy()

    if "residual_dev" not in df.columns:
        df["residual_dev"] = (df["residual"] - df["residual_ewma"]).to_numpy(dtype=float)

    # Standardized residuals (fallback: compute baseline from first 30%)
    if "residual_z" not in df.columns:
        base_n = max(1000, int(0.3 * len(df)))
        base = pd.to_numeric(df.loc[: base_n - 1, "residual"], errors="coerce")
        mu = float(np.nanmean(base))
        sig = float(np.nanstd(base))
        if not np.isfinite(sig) or sig < 1e-9:
            sig = 1.0
        df["residual_z"] = ((df["residual"] - mu) / sig).to_numpy(dtype=float)

    if "residual_z_ewma" not in df.columns:
        df["residual_z_ewma"] = df["residual_z"].ewm(span=30, adjust=False).mean().to_numpy()

    if "residual_z_dev" not in df.columns:
        df["residual_z_dev"] = (df["residual_z"] - df["residual_z_ewma"]).to_numpy(dtype=float)

    # MSPC indices
    for c in ["t2", "spe"]:
        if c not in df.columns:
            df[c] = np.nan

    # Limits (fallback: use 99th percentile)
    if "t2_limit" not in df.columns:
        df["t2_limit"] = float(np.nanquantile(pd.to_numeric(df["t2"], errors="coerce"), 0.99)) if df["t2"].notna().any() else np.nan
    if "spe_limit" not in df.columns:
        df["spe_limit"] = float(np.nanquantile(pd.to_numeric(df["spe"], errors="coerce"), 0.99)) if df["spe"].notna().any() else np.nan

    if "residual_z_limit" not in df.columns:
        df["residual_z_limit"] = float(np.nanquantile(np.abs(pd.to_numeric(df["residual_z"], errors="coerce")), 0.99))
    if "residual_z_dev_limit" not in df.columns:
        df["residual_z_dev_limit"] = float(np.nanquantile(np.abs(pd.to_numeric(df["residual_z_dev"], errors="coerce")), 0.99))

    # Scores + health
    if "mspc_score" not in df.columns:
        t2_ratio = pd.to_numeric(df["t2"], errors="coerce") / pd.to_numeric(df["t2_limit"], errors="coerce").replace({0.0: np.nan})
        spe_ratio = pd.to_numeric(df["spe"], errors="coerce") / pd.to_numeric(df["spe_limit"], errors="coerce").replace({0.0: np.nan})
        df["mspc_score"] = np.nanmax(np.vstack([t2_ratio.to_numpy(), spe_ratio.to_numpy()]), axis=0)

    if "residual_score" not in df.columns:
        z = np.abs(pd.to_numeric(df["residual_z"], errors="coerce")) / pd.to_numeric(df["residual_z_limit"], errors="coerce").replace({0.0: np.nan})
        zdev = np.abs(pd.to_numeric(df["residual_z_dev"], errors="coerce")) / pd.to_numeric(df["residual_z_dev_limit"], errors="coerce").replace({0.0: np.nan})
        df["residual_score"] = np.nanmax(np.vstack([z.to_numpy(), zdev.to_numpy()]), axis=0)

    if "anomaly_score" not in df.columns:
        df["anomaly_score"] = np.nanmax(np.vstack([df["residual_score"].to_numpy(), df["mspc_score"].to_numpy()]), axis=0)

    if "anomaly_ewma" not in df.columns:
        df["anomaly_ewma"] = pd.Series(df["anomaly_score"]).ewm(span=30, adjust=False).mean().to_numpy()

    if "health" not in df.columns:
        df["health"] = (1.0 / (1.0 + df["anomaly_ewma"]).replace({np.inf: np.nan})).to_numpy(dtype=float)

    if "health_lower" not in df.columns or "health_upper" not in df.columns:
        sigma = pd.Series(df["anomaly_score"]).ewm(span=60, adjust=False).std(bias=False)
        sigma = sigma.fillna(0.0)
        a = pd.Series(df["anomaly_ewma"]).to_numpy()
        s = sigma.to_numpy()
        df["health_lower"] = 1.0 / (1.0 + (a + s))
        df["health_upper"] = 1.0 / (1.0 + np.maximum(a - s, 0.0))

    # alarms
    if "residual_alarm" not in df.columns:
        df["residual_alarm"] = (np.abs(df["residual_z"]) > df["residual_z_limit"]).astype(bool)

    if "mspc_alarm" not in df.columns:
        df["mspc_alarm"] = (df["mspc_score"] > 1.0).astype(bool)

    if "alarm_any" not in df.columns:
        df["alarm_any"] = (df["residual_alarm"] | df["mspc_alarm"]).astype(bool)

    if "root_cause" not in df.columns:
        df["root_cause"] = np.nan

    return df


def list_scores_files() -> list[Path]:
    return sorted([p for p in OUTPUTS_DIR.glob("*.csv") if p.is_file()])


def _pretty_scores_file_label(p: Path) -> str:
    name = p.name
    low = name.lower()

    if low == "scores.csv":
        return "Single model (legacy alias) — scores.csv"
    if "ensemble" in low:
        return f"Ensemble model — {name}"
    if "single" in low:
        return f"Single model — {name}"
    if "latest" in low:
        return f"Latest computed — {name}"
    return name


def _mode(series: pd.Series):
    s = series.dropna()
    if s.empty:
        return np.nan
    return s.value_counts().idxmax()


def downsample(df: pd.DataFrame, rule: str) -> pd.DataFrame:
    """Downsample time series for plotting."""
    if rule in ("None", "Raw"):
        return df.copy()

    df = df.copy()
    df = df.set_index("timestamp")

    bool_cols = [c for c in ["residual_alarm", "mspc_alarm", "alarm_any"] if c in df.columns]
    cat_cols = [c for c in ["root_cause", "fault_label"] if c in df.columns]
    num_cols = [c for c in df.columns if c not in bool_cols + cat_cols]

    agg = {}
    for c in num_cols:
        agg[c] = "mean"
    for c in bool_cols:
        agg[c] = "max"
    for c in cat_cols:
        agg[c] = _mode

    out = df.resample(rule).agg(agg).reset_index()
    return _ensure_scores_schema(out)


def detect_incidents(mask: pd.Series, min_points: int = 10) -> list[tuple[pd.Timestamp, pd.Timestamp]]:
    """Return list of (start,end) for contiguous True segments of at least min_points."""
    mask = mask.fillna(False).astype(bool).to_numpy()
    if mask.size == 0:
        return []

    runs: list[tuple[int, int]] = []
    start = None
    for i, v in enumerate(mask):
        if v and start is None:
            start = i
        elif (not v) and start is not None:
            end = i - 1
            runs.append((start, end))
            start = None
    if start is not None:
        runs.append((start, mask.size - 1))

    out: list[tuple[pd.Timestamp, pd.Timestamp]] = []
    for s, e in runs:
        if (e - s + 1) >= int(min_points):
            out.append((s, e))
    return out


def incident_table(df_plot: pd.DataFrame, min_points_active: int = 10) -> pd.DataFrame:
    if df_plot.empty:
        return pd.DataFrame(columns=["start", "end", "points", "max_anomaly", "top_root_cause"])

    mask = pd.Series(df_plot.get("anomaly_score", pd.Series([np.nan] * len(df_plot)))) > 1.0
    runs = detect_incidents(mask, min_points=min_points_active)

    rows = []
    for s, e in runs:
        chunk = df_plot.iloc[s : e + 1]
        rows.append(
            {
                "start": chunk["timestamp"].iloc[0],
                "end": chunk["timestamp"].iloc[-1],
                "points": len(chunk),
                "max_anomaly": float(np.nanmax(chunk["anomaly_score"])) if "anomaly_score" in chunk else np.nan,
                "top_root_cause": _mode(chunk.get("root_cause", pd.Series([], dtype=object))),
            }
        )

    out = pd.DataFrame(rows)
    return out.sort_values("start", ascending=False).reset_index(drop=True)

def _infer_step_minutes(df: pd.DataFrame) -> float | None:
    if df is None or df.empty or "timestamp" not in df.columns:
        return None
    d = df["timestamp"].sort_values().diff().dropna()
    if d.empty:
        return None
    sec = float(d.dt.total_seconds().median())
    if not np.isfinite(sec) or sec <= 0:
        return None
    return sec / 60.0


def _pretty_root_cause(name: object, step_min: float | None) -> str:
    if not isinstance(name, str) or not name:
        return "-"
    low = name.lower()

    m = re.match(r"^(?P<tag>.+?)_lag_(?P<n>\d+)$", low)
    if m:
        tag = m.group("tag").replace("_", " ")
        n = int(m.group("n"))
        if step_min:
            minutes = n * step_min
            if minutes >= 60:
                return f"{tag} (lag {n} ≈ {minutes/60:.1f} h)"
            return f"{tag} (lag {n} ≈ {minutes:.0f} min)"
        return f"{tag} (lag {n})"

    m = re.match(r"^(?P<tag>.+?)_roll_(?P<n>\d+)$", low)
    if m:
        tag = m.group("tag").replace("_", " ")
        n = int(m.group("n"))
        if step_min:
            minutes = n * step_min
            if minutes >= 60:
                return f"{tag} (roll {n} ≈ {minutes/60:.1f} h)"
            return f"{tag} (roll {n} ≈ {minutes:.0f} min)"
        return f"{tag} (roll {n})"

    return name.replace("_", " ")

def apply_legend_top(fig: go.Figure) -> go.Figure:
    """
    Move legend to the top so it doesn't cover x-axis labels.
    Works well for wide time-series plots.
    """
    fig.update_layout(
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="left",
            x=0.0,
        ),
        margin=dict(t=70),  # ensure space for legend
    )
    return fig


# -------------------- Plotting --------------------
def plot_actual_vs_pred(df_plot: pd.DataFrame) -> go.Figure:
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=df_plot["y_actual"], mode="lines", name="Actual"))
    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=df_plot["y_pred"], mode="lines", name="Predicted"))
    fig.update_layout(height=300, margin=dict(l=20, r=20, t=30, b=20), legend=dict(orientation="h"))
    fig.update_xaxes(title=None)
    fig.update_yaxes(title=None)
    return fig


def plot_residual(df_plot: pd.DataFrame) -> go.Figure:
    """Residual / error chart.

    The original residual+EWMA plot can look "flat" even when a model is bad
    (e.g., consistently biased). What you usually want to monitor is the
    *magnitude* of prediction error.

    We therefore plot absolute error + EWMA, and an empirical p99 "alert" line.
    """
    resid = pd.to_numeric(df_plot.get("residual"), errors="coerce")
    abs_err = resid.abs()
    abs_ew = abs_err.ewm(span=30, adjust=False).mean()

    # Prefer a stored limit if available, otherwise compute a robust empirical one
    lim = (
        float(pd.to_numeric(df_plot.get("residual_abs_limit"), errors="coerce").iloc[0])
        if "residual_abs_limit" in df_plot.columns
        else float(np.nanquantile(abs_err.to_numpy(dtype=float), 0.99))
        if abs_err.notna().any()
        else np.nan
    )

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=abs_err, mode="lines", name="Abs error"))
    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=abs_ew, mode="lines", name="Abs error EWMA"))

    if np.isfinite(lim) and lim > 0:
        fig.add_hline(y=lim, line_dash="dot", annotation_text=f"Alert (p99≈{lim:.2f})", annotation_position="top left")

    # auto-zoom y (start at 0 for readability)
    y_min = 0.0
    y_max = float(np.nanmax([abs_err.max(), abs_ew.max(), lim if np.isfinite(lim) else 0.0]))
    pad = 0.1 * (y_max - y_min + 1e-9)

    fig.update_layout(height=260, margin=dict(l=20, r=20, t=30, b=20), legend=dict(orientation="h"))
    fig.update_yaxes(range=[y_min, y_max + pad], title="Absolute error")
    fig.update_xaxes(title=None)
    return fig


def plot_health(df_plot: pd.DataFrame) -> go.Figure:
    """Health score with uncertainty band.

    health = 1/(1+anomaly_score_EWMA). 1 is healthy, 0 is bad.
    We plot an intuitive uncertainty band derived from EWMA std.
    """
    h = pd.to_numeric(df_plot.get("health"), errors="coerce")
    lo = pd.to_numeric(df_plot.get("health_lower"), errors="coerce")
    hi = pd.to_numeric(df_plot.get("health_upper"), errors="coerce")

    fig = go.Figure()

    # band
    fig.add_trace(
        go.Scatter(
            x=df_plot["timestamp"],
            y=hi,
            mode="lines",
            line=dict(width=0),
            showlegend=False,
            hoverinfo="skip",
        )
    )
    fig.add_trace(
        go.Scatter(
            x=df_plot["timestamp"],
            y=lo,
            mode="lines",
            line=dict(width=0),
            fill="tonexty",
            name="Uncertainty band",
            hoverinfo="skip",
        )
    )

    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=h, mode="lines", name="Health"))

    # threshold corresponding to anomaly_score = 1.0  => health = 0.5
    fig.add_hline(y=0.5, line_dash="dot", annotation_text="Threshold (anomaly=1)", annotation_position="bottom left")

    fig.update_layout(height=260, margin=dict(l=20, r=20, t=30, b=20), legend=dict(orientation="h"))
    fig.update_yaxes(range=[0, 1], title="Health (1=good)")
    fig.update_xaxes(title=None)
    return fig


def plot_mspc_ratio(df_plot: pd.DataFrame) -> go.Figure:
    """MSPC chart as ratio-to-limit (fixes the 'SPE is 0' confusion)."""
    t2 = pd.to_numeric(df_plot.get("t2"), errors="coerce")
    spe = pd.to_numeric(df_plot.get("spe"), errors="coerce")

    t2_lim = float(pd.to_numeric(df_plot.get("t2_limit"), errors="coerce").iloc[0]) if "t2_limit" in df_plot.columns else float(np.nanquantile(t2, 0.99))
    spe_lim = float(pd.to_numeric(df_plot.get("spe_limit"), errors="coerce").iloc[0]) if "spe_limit" in df_plot.columns else float(np.nanquantile(spe, 0.99))

    if not np.isfinite(t2_lim) or t2_lim <= 0:
        t2_lim = 1.0
    if not np.isfinite(spe_lim) or spe_lim <= 0:
        spe_lim = 1.0

    t2_ratio = t2 / t2_lim
    spe_ratio = spe / spe_lim

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=t2_ratio, mode="lines", name="T² / limit"))
    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=spe_ratio, mode="lines", name="SPE/Q / limit"))
    fig.add_hline(y=1.0, line_dash="dot", annotation_text="Alarm threshold (1.0)", annotation_position="top left")

    # auto zoom
    y_min = float(np.nanmin([t2_ratio.min(), spe_ratio.min(), 0.0]))
    y_max = float(np.nanmax([t2_ratio.max(), spe_ratio.max(), 1.0]))
    pad = 0.1 * (y_max - y_min + 1e-9)

    fig.update_layout(height=260, margin=dict(l=20, r=20, t=30, b=20), legend=dict(orientation="h"))
    fig.update_yaxes(range=[y_min - pad, y_max + pad], title="Ratio-to-limit")
    fig.update_xaxes(title=None)
    return fig


def plot_control_chart(df_plot: pd.DataFrame, value_col: str, critical_threshold: float = 3.0) -> go.Figure:
    x = pd.to_numeric(df_plot.get(value_col), errors="coerce")
    mu = float(np.nanmean(x))
    sigma = float(np.nanstd(x))
    if not np.isfinite(sigma) or sigma < 1e-9:
        sigma = 1.0

    cl = mu
    ucl = mu + 3 * sigma
    lcl = mu - 3 * sigma

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df_plot["timestamp"], y=x, mode="lines", name=value_col))

    fig.add_hline(y=cl, line_dash="dash", annotation_text="CL", annotation_position="bottom left")
    fig.add_hline(y=ucl, line_dash="dot", annotation_text="UCL", annotation_position="top left")
    fig.add_hline(y=lcl, line_dash="dot", annotation_text="LCL", annotation_position="bottom left")
    fig.add_hline(y=float(critical_threshold), line_dash="dash", annotation_text=f"Critical threshold ({critical_threshold:g})", annotation_position="top left")

    # Auto zoom (requested)
    y_min = float(np.nanmin([x.min(), lcl]))
    y_max = float(np.nanmax([x.max(), ucl]))
    pad = 0.1 * (y_max - y_min + 1e-9)

    fig.update_layout(height=280, margin=dict(l=20, r=20, t=30, b=20), showlegend=False)
    fig.update_yaxes(range=[y_min - pad, y_max + pad], title=value_col)
    fig.update_xaxes(title=None)
    return fig


def plot_failure_trajectory(
    df_plot: pd.DataFrame,
    sensitivity: float = 1.0,
    end_condition: float = 10.0,
) -> go.Figure:
    """Build a dynamic equipment failure trajectory with symptom annotations.

    We derive a monotonic "condition" from the anomaly score:
    - When anomaly <= 1: no degradation.
    - When anomaly > 1: degradation accumulates.

    This is a *demo visualization* to communicate the trajectory concept.
    """
    ts = df_plot["timestamp"]
    anom = pd.to_numeric(df_plot.get("anomaly_ewma", df_plot.get("anomaly_score")), errors="coerce").fillna(0.0).to_numpy()

    # Damage accumulates only when anomaly exceeds 1.0
    damage = np.maximum(0.0, anom - 1.0) * float(sensitivity)
    cum = np.cumsum(damage)

    if np.nanmax(cum) <= 1e-12:
        condition = np.ones_like(cum) * 100.0
    else:
        # Scale so the window ends at `end_condition` (keeps plot useful even if anomalies are mild)
        condition = 100.0 - (100.0 - float(end_condition)) * (cum / np.nanmax(cum))

    # Symptom thresholds (approximate, ordered)
    symptoms = [
        (92, "Ultrasound detected"),
        (84, "Vibration analysis fault"),
        (76, "Oil analysis detected"),
        (68, "Audible noise"),
        (60, "Hot to touch"),
        (52, "Mechanically loose"),
        (44, "Ancillary damage"),
        (float(end_condition), "Alarm or acknowledgement"),
    ]

    # Find first crossing time for each threshold
    pts_x = []
    pts_y = []
    pts_lbl = []
    for thr, lbl in symptoms:
        idx = np.where(condition <= thr + 1e-9)[0]
        if idx.size:
            i = int(idx[0])
            pts_x.append(ts.iloc[i])
            pts_y.append(float(condition[i]))
            pts_lbl.append(lbl)

    fig = go.Figure()

    fig.add_trace(go.Scatter(x=ts, y=condition, mode="lines", name="Condition"))

    # symptom markers
    if pts_x:
        fig.add_trace(
            go.Scatter(
                x=pts_x,
                y=pts_y,
                mode="markers",
                name="Symptoms",
                marker=dict(size=8),
                text=pts_lbl,
                hovertemplate="%{text}<br>%{x}<br>Condition=%{y:.1f}<extra></extra>",
            )
        )

        # annotate a subset (avoid clutter)
        for x, y, lbl in zip(pts_x[:6], pts_y[:6], pts_lbl[:6]):
            fig.add_annotation(x=x, y=y, text=lbl, showarrow=True, arrowhead=2, ax=20, ay=-30)

    # Point P and Point F
    fig.add_annotation(x=ts.iloc[0], y=float(condition[0]), text="Point P", showarrow=True, arrowhead=2, ax=0, ay=-40)
    fig.add_annotation(x=ts.iloc[-1], y=float(condition[-1]), text="Point F", showarrow=True, arrowhead=2, ax=0, ay=-40)

    fig.update_layout(height=340, margin=dict(l=20, r=20, t=30, b=20), legend=dict(orientation="h"))
    fig.update_yaxes(title="Equipment condition (%)", range=[0, 105])
    fig.update_xaxes(title=None)
    return fig


# -------------------- Streamlit App --------------------
st.set_page_config(page_title="Hydrocracker OFM Dashboard", layout="wide")

st.title("Hydrocracker OFM Dashboard (Fuel Gas Soft Sensor + MSPC)")

# Sidebar controls
with st.sidebar:
    st.header("Controls")

    # Data source selection
    sources = []
    if "computed_scores" in st.session_state:
        sources.append("Latest computed (this session)")
    sources.extend(["Local outputs (.csv)", "Upload scores CSV"])

    source = st.selectbox("Scores source", options=sources)

    selected_file: Optional[Path] = None
    uploaded_file = None

    if source == "Local outputs (.csv)":
        files = list_scores_files()
        if not files:
            st.info("No score CSVs found under ./outputs. Generate one via the pipelines, or upload a CSV.")
        else:
            selected_file = st.selectbox("Select a scores CSV", options=files, format_func=_pretty_scores_file_label)

    if source == "Upload scores CSV":
        uploaded_file = st.file_uploader("Upload a scores CSV", type=["csv"])

    if source == "Latest computed (this session)":
        st.success("Using in-memory scores from the Build/Score tab.")
        if st.button("Revert / clear computed scores"):
            st.session_state.pop("computed_scores", None)
            st.session_state.pop("computed_scores_name", None)
            st.rerun()

    st.divider()

    downsample_rule = st.selectbox(
        "Downsample for plotting",
        options=["Raw", "5min", "15min", "1H", "6H", "1D"],
        index=3,
        help="For long spans, downsample to keep plots responsive.",
    )

    min_points_active = st.number_input(
        "Min points to call an incident ACTIVE (else POTENTIAL)",
        min_value=1,
        value=10,
        step=1,
    )

    critical_threshold = st.number_input(
        "CRITICAL threshold (Control Chart)",
        min_value=0.1,
        value=3.0,
        step=0.1,
    )


# Tabs: Monitor + Build
monitor_tab, build_tab = st.tabs(["Monitor", "Build / Score new data"])


# -------------------- Build tab --------------------
with build_tab:
    st.subheader("Build / Score new data")
    st.write(
        "Upload a *dataset CSV* and the two model bundles (*.joblib). The dashboard will run scoring in-memory "
        "and make the result available under **Latest computed (this session)** in the sidebar."
    )

    c1, c2, c3 = st.columns(3)
    with c1:
        up_dataset = st.file_uploader("Dataset CSV (same as pipelines input)", type=["csv"], key="up_dataset")
    with c2:
        up_soft = st.file_uploader("Soft-sensor model bundle (.joblib)", type=["joblib"], key="up_soft")
    with c3:
        up_ofm = st.file_uploader("OFM model bundle (.joblib)", type=["joblib"], key="up_ofm")

    save_name = st.text_input("Optional: save computed scores to ./outputs as", value="scores_latest.csv")

    if st.button("Run scoring", type="primary", disabled=not (up_dataset and up_soft and up_ofm)):
        with st.spinner("Scoring..."):
            ds = pd.read_csv(up_dataset)
            soft_bundle = joblib.load(io.BytesIO(up_soft.getvalue()))
            ofm_bundle = joblib.load(io.BytesIO(up_ofm.getvalue()))
            scored = score_dataframe(ds, soft_bundle, ofm_bundle)

        st.session_state["computed_scores"] = scored
        st.session_state["computed_scores_name"] = "computed_scores"
        st.success(f"Scored {len(scored):,} rows. Switch to the Monitor tab or pick 'Latest computed' in the sidebar.")

        if save_name:
            out_path = OUTPUTS_DIR / save_name
            scored.to_csv(out_path, index=False)
            st.info(f"Saved: {out_path}")

    st.markdown("---")
    st.write("Terminal workflow (recommended for repeatable runs):")
    st.code(
        "python -m ofm_fg_ofm.pipelines.make_dataset --generate-synthetic --months 5 --freq 5min --out data/processed/dataset.csv\n"
        "python -m ofm_fg_ofm.pipelines.train_soft_sensor --data data/processed/dataset.csv --out models/soft_sensor.joblib\n"
        "python -m ofm_fg_ofm.pipelines.train_ofm --data data/processed/dataset.csv --soft models/soft_sensor.joblib --out models/ofm.joblib\n"
        "python -m ofm_fg_ofm.pipelines.score_ofm --data data/processed/dataset.csv --soft models/soft_sensor.joblib --ofm models/ofm.joblib --out outputs/scores.csv\n"
        "streamlit run dashboard/app.py"
    )


# -------------------- Load selected scores (for Monitor) --------------------
raw_scores: Optional[pd.DataFrame] = None
source_label = ""

if source == "Latest computed (this session)" and "computed_scores" in st.session_state:
    raw_scores = st.session_state["computed_scores"].copy()
    source_label = "Latest computed (session)"
elif source == "Local outputs (.csv)" and selected_file is not None:
    raw_scores = load_scores_csv(selected_file)
    source_label = f"Local: {_pretty_scores_file_label(selected_file)}"
elif source == "Upload scores CSV" and uploaded_file is not None:
    raw_scores = load_scores_uploaded(uploaded_file)
    source_label = f"Uploaded: {uploaded_file.name}"


# -------------------- Monitor tab --------------------
with monitor_tab:
    st.subheader("Monitor")

    if raw_scores is None or raw_scores.empty:
        st.warning("No scores loaded yet.")
        st.write(
            "Generate scores using the pipelines (see Build tab) or upload a scores CSV. "
            "The dashboard expects at minimum: timestamp, y_actual, y_pred (or residual), plus optional MSPC columns." 
        )
        st.stop()

    st.caption(f"Source: {source_label}")

    # Window selector
    ts_min = raw_scores["timestamp"].min()
    ts_max = raw_scores["timestamp"].max()

    window = st.slider("Select window", min_value=ts_min.to_pydatetime(), max_value=ts_max.to_pydatetime(), value=(ts_min.to_pydatetime(), ts_max.to_pydatetime()))
    start_ts = pd.Timestamp(window[0])
    end_ts = pd.Timestamp(window[1])

    df_window = raw_scores[(raw_scores["timestamp"] >= start_ts) & (raw_scores["timestamp"] <= end_ts)].reset_index(drop=True)

    if df_window.empty:
        st.info("No rows in the selected window.")
        st.stop()

    df_plot = downsample(df_window, downsample_rule)
    inc = incident_table(df_plot, min_points_active=int(min_points_active))
    step_min = _infer_step_minutes(raw_scores)

    if not inc.empty:
        inc["top_root_cause"] = inc["top_root_cause"].apply(lambda x: _pretty_root_cause(x, step_min))

    # Metrics
    y = pd.to_numeric(df_window.get("y_actual"), errors="coerce")
    yhat = pd.to_numeric(df_window.get("y_pred"), errors="coerce")
    err = y - yhat
    mae = float(np.nanmean(np.abs(err))) if err.notna().any() else np.nan
    max_ae = float(np.nanmax(np.abs(err))) if err.notna().any() else np.nan
    rmse = float(np.sqrt(np.nanmean(err**2))) if err.notna().any() else np.nan

    c1, c2, c3, c4 = st.columns(4)
    with c1:
        st.metric("Rows in window (raw)", f"{len(df_window):,}")
        st.metric("Rows plotted", f"{len(df_plot):,}")
    with c2:
        st.metric("Residual alarms", f"{int(df_plot.get('residual_alarm', pd.Series([False]*len(df_plot))).sum()):,}")
        st.metric("MSPC alarms", f"{int(df_plot.get('mspc_alarm', pd.Series([False]*len(df_plot))).sum()):,}")
    with c3:
        active = (pd.to_numeric(df_plot.get("anomaly_score"), errors="coerce") > 1.0).sum()
        st.metric("Active points (anomaly>1.0)", f"{int(active):,}")
        st.metric("Detected incidents", f"{len(inc):,}")
    with c4:
        st.metric("MAE", f"{mae:.4g}" if np.isfinite(mae) else "-")
        st.metric("RMSE", f"{rmse:.4g}" if np.isfinite(rmse) else "-")

    st.markdown("---")

    # Soft sensor
    st.subheader("Fuel Gas Soft Sensor")
    cmae, cmax, crmse = st.columns(3)
    with cmae:
        st.metric("Mean abs error", f"{mae:.4g}" if np.isfinite(mae) else "-")
    with cmax:
        st.metric("Max abs error", f"{max_ae:.4g}" if np.isfinite(max_ae) else "-")
    with crmse:
        st.metric("RMSE", f"{rmse:.4g}" if np.isfinite(rmse) else "-")

    st.plotly_chart(apply_legend_top(plot_actual_vs_pred(df_plot)), use_container_width=True)

    # Residual chart
    st.plotly_chart(apply_legend_top(plot_residual(df_plot)), use_container_width=True)

    # Health
    st.subheader("Equipment Health")
    st.plotly_chart(apply_legend_top(plot_health(df_plot)), use_container_width=True)

    # Failure trajectory
    st.subheader("Equipment Failure Trajectory")
    sens = st.slider("Trajectory sensitivity", min_value=0.2, max_value=5.0, value=1.0, step=0.1)
    end_cond = st.slider("End-of-window condition (%)", min_value=0.0, max_value=50.0, value=10.0, step=1.0)
    st.plotly_chart(
        apply_legend_top(plot_failure_trajectory(df_plot, sensitivity=sens, end_condition=end_cond)),
        use_container_width=True,
    )

    # MSPC indices
    st.subheader("MSPC Indices")
    st.plotly_chart(apply_legend_top(plot_mspc_ratio(df_plot)), use_container_width=True)

    # Control chart
    st.subheader("Control Chart with Stability Analysis")
    st.plotly_chart(
        apply_legend_top(
            plot_control_chart(df_plot, value_col="anomaly_score", critical_threshold=critical_threshold)
        ),
        use_container_width=True,
    )

    # Incidents
    st.subheader("Detected Incidents")
    if inc.empty:
        st.info("No incidents detected (anomaly_score > 1.0 with persistence).")
    else:
        st.caption(
            "top_root_cause = the most frequent MSPC SPE-contribution driver inside the incident window "
            "(i.e., which engineered feature deviated most from the multivariate PCA model)."
        )
        st.dataframe(inc, use_container_width=True)

